---
title: How I Used a Transformer Model to Predict Conway's Game of Life in Python
---
As an aspiring young IT professional primarily focused on C++, I’ve always been fascinated by the rapid advancements in machine learning and artificial intelligence. While my day-to-day work involves coding and problem-solving in C++, I knew that to stay relevant and push my boundaries, I needed to explore other areas of technology. This curiosity led me to embark on a project that was completely outside my comfort zone: building a model to predict the next state in Conway's Game of Life using a Transformer architecture. This blog post is a recount of my journey, the challenges I faced, and the lessons I learned along the way.

### **Why Conway’s Game of Life?**

Conway's Game of Life is a cellular automaton that has intrigued computer scientists for decades. Despite its simple rules, the game can produce highly complex and unpredictable patterns, making it an ideal candidate for experimenting with machine learning models. The idea of training a model to predict these patterns was appealing, especially since I wanted to deepen my understanding of sequence modeling with Transformers—an architecture initially designed for natural language processing but which I believed could be equally powerful in this context.

### **Setting Up the Project**

Given my background in C++, this project was an opportunity to explore Python, a language widely regarded as the go-to for machine learning projects due to its vast array of libraries and ease of use. To set up my environment, I installed the following essential libraries:

```bash
pip install torch numpy matplotlib
```

These libraries provided me with the tools needed to build, train, and visualize my model.

### **Data Generation**

The first challenge I faced was generating the data required to train my model. Each grid in Conway’s Game of Life is a 16x16 binary matrix where `1` represents a live cell and `0` represents a dead cell. I needed sequences of these grids, where each subsequent grid state was generated by applying the Game of Life rules to the previous state.

Here’s a snippet of the code I used to generate the dataset:

```python
import numpy as np

def generate_initial_grid(size=16):
    return np.random.choice([0, 1], size=(size, size))

def game_of_life_step(grid):
    neighbors = sum(np.roll(np.roll(grid, i, 0), j, 1)
                    for i in (-1, 0, 1) for j in (-1, 0, 1)
                    if (i, j) != (0, 0))
    return (neighbors == 3) | (grid & (neighbors == 2))

def generate_sequence(initial_grid, steps=10):
    sequence = [initial_grid]
    grid = initial_grid.copy()
    for _ in range(steps):
        grid = game_of_life_step(grid)
        sequence.append(grid)
    return sequence

dataset = create_dataset(num_samples=5000, grid_size=16, sequence_length=10)
np.save("game_of_life_dataset.npy", dataset)
```

This code was responsible for generating a sequence of grid states based on the initial random state, which would then be used to train my Transformer model.

### **Building the Transformer Model**

Designing the Transformer model was the next step in my journey. The key idea was to treat each grid as a sequence, where each cell’s state is influenced by its neighbors, much like how words in a sentence influence each other in language models. Using PyTorch, I built a Transformer model with a focus on simplicity and effectiveness.

Here’s the core of my Transformer model:

```python
import torch
import torch.nn as nn

class TransformerModel(nn.Module):
    def __init__(self, grid_size, num_heads, num_layers, d_model, dim_feedforward):
        super(TransformerModel, self).__init__()
        self.embedding = nn.Linear(grid_size * grid_size, d_model)
        self.positional_encoding = nn.Parameter(torch.zeros(1, 1, d_model))
        encoder_layer = nn.TransformerEncoderLayer(d_model=d_model, nhead=num_heads, dim_feedforward=dim_feedforward, batch_first=True)
        self.transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)
        self.fc_out = nn.Linear(d_model, grid_size * grid_size)

    def forward(self, x):
        x = self.embedding(x) + self.positional_encoding
        x = self.transformer_encoder(x)
        x = self.fc_out(x)
        return torch.sigmoid(x)
```

In this model, each grid is first embedded into a higher-dimensional space. The embedded grid is then passed through several layers of the Transformer encoder, which processes it and outputs a prediction for the next grid state.

### **Training the Model**

Training a Transformer model to accurately predict the next state in Conway’s Game of Life proved to be a significant challenge. I initially struggled with overfitting and unstable training, but I managed to address these issues by incorporating a learning rate scheduler and an early stopping mechanism. Below is the training loop that I settled on:

```python
def train_model(model, train_loader, num_epochs=100, learning_rate=0.0001):
    criterion = nn.BCELoss()
    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)
    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=5, verbose=True)
    
    for epoch in range(num_epochs):
        epoch_loss = 0.0
        for inputs, targets in train_loader:
            optimizer.zero_grad()
            outputs = model(inputs)
            loss = criterion(outputs, targets)
            loss.backward()
            optimizer.step()
            epoch_loss += loss.item()
        
        average_loss = epoch_loss / len(train_loader)
        print(f"Epoch [{epoch+1}/{num_epochs}], Loss: {average_loss:.4f}")
        
        scheduler.step(average_loss)
        
        if average_loss < 0.001:
            print("Early stopping as the model has reached near-perfect accuracy.")
            break
```

### **Challenges and Learnings**

One of the most significant challenges was ensuring that the model didn’t simply memorize the training data but could generalize well to unseen sequences. This required careful fine-tuning of the model’s hyperparameters and the implementation of techniques like learning rate scheduling to help the model converge effectively.

### **Results and Visualization**

After several rounds of tweaking and experimenting, the model began to perform impressively well, often predicting the next state with high accuracy. To help me visualize and debug the model’s predictions, I used Matplotlib. This was invaluable for understanding where the model was making errors and how it was improving.

Here’s an example of how I visualized the sequence predictions:

```python
import matplotlib.pyplot as plt

def plot_sequence(sequence):
    fig, axs = plt.subplots(1, len(sequence), figsize=(15, 15))
    for i, grid in enumerate(sequence):
        axs[i].imshow(grid.reshape(16, 16), cmap='binary', vmin=0, vmax=1)
        axs[i].grid(True, which='both', color='lightgray', linewidth=0.5)
        axs[i].set_xticks(np.arange(-.5, 16, 1))
        axs[i].set_yticks(np.arange(-.5, 16, 1))
        axs[i].set_xticklabels([])
        axs[i].set_yticklabels([])
        axs[i].axis('off')
        axs[i].set_title(f"Iteration {i+1}", fontsize=12, loc='left', pad=10)
    plt.tight_layout()
    plt.show()
```

### **Conclusion**

This project was a deep dive into machine learning, pushing me to expand my skill set beyond C++. It was a rewarding experience that not only enhanced my understanding of Transformer models but also showed me how versatile these models can be—even for tasks outside of their original design, like predicting cellular automata patterns.

Looking ahead, I’m already considering my next project—possibly tackling a similar problem but implementing it in C++ to combine my newfound knowledge with my existing skills. If you’re interested in exploring sequence models or simply want to challenge yourself, I highly recommend embarking on a project like this. It’s a fantastic way to learn, experiment, and grow as a developer.

### **References**

- PyTorch Documentation: [https://pytorch.org/docs/stable/index.html](https://pytorch.org/docs/stable/index.html)
- Transformer Model Architecture: [https://arxiv.org/abs/1706.03762](https://arxiv.org/abs/1706.03762)
- Conway’s Game of Life: [https://en.wikipedia.org/wiki/Conway%27s_Game_of_Life](https://en.wikipedia.org/wiki/Conway%27s_Game_of_Life)

Feel free to reach out if you have any questions or want to share your own experiences with similar projects!